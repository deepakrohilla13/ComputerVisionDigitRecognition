{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "('Training data shape:', (230070, 32, 32, 1))\n",
      "('Training label shape:', (230070, 6))\n",
      "('Validation data shape:', (5684, 32, 32, 1))\n",
      "('Validation label shape:', (5684, 6))\n",
      "('Test data shape:', (13068, 32, 32, 1))\n",
      "('Test label shape:', (13068, 6))\n",
      "Data successfully loaded!\n",
      "Defining accuracy function...\n",
      "Accuracy function defined!\n",
      "Loading data and building computation graph...\n",
      "Final image size after convolutions 1\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:209 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:239 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:240 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:242 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:243 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:245 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:246 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:248 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:249 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:251 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:252 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:254 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:255 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:257 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:258 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:260 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:261 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:263 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:264 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "Data loaded and computation graph built!\n",
      "Running computation and iteration...\n",
      "If you are unable to save the summary, please change the path to where you want it to write.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:274 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:275 in <module>.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda2/envs/p/lib/python2.7/site-packages/tensorflow/python/ops/logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From <ipython-input-1-92777c3694b7>:281 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 19.020236969\n",
      "Minibatch accuracy: 6.25%\n",
      "Validation accuracy: 43.7684729064%\n",
      "Minibatch loss at step 500: 6.92303848267\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 1000: 6.539914608\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 1500: 6.34905147552\n",
      "Minibatch accuracy: 61.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 2000: 6.44088745117\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 2500: 6.75622797012\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 3000: 6.55183887482\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.2660098522%\n",
      "Minibatch loss at step 3500: 7.28228902817\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 4000: 7.31090497971\n",
      "Minibatch accuracy: 48.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 4500: 6.3328576088\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 5000: 7.38145971298\n",
      "Minibatch accuracy: 48.75%\n",
      "Validation accuracy: 57.2660098522%\n",
      "Minibatch loss at step 5500: 7.0385222435\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 6000: 6.99745082855\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 6500: 7.22429656982\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.2660098522%\n",
      "Minibatch loss at step 7000: 7.08954381943\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 7500: 6.66203546524\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 8000: 6.83566951752\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 8500: 6.69647264481\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 9000: 6.43428182602\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 9500: 6.99593162537\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 10000: 6.52022838593\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.2660098522%\n",
      "Minibatch loss at step 10500: 6.05630636215\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 11000: 6.47624254227\n",
      "Minibatch accuracy: 58.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 11500: 7.25150585175\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 12000: 7.01277732849\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 12500: 6.38214874268\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 13000: 6.18558931351\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 13500: 6.2557516098\n",
      "Minibatch accuracy: 61.25%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 14000: 6.76328992844\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 14500: 6.67467927933\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 15000: 6.84643650055\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 15500: 6.91445159912\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 16000: 6.25171947479\n",
      "Minibatch accuracy: 61.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 16500: 6.81567192078\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 17000: 7.06349945068\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 17500: 6.77363443375\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 18000: 6.90312719345\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 18500: 6.94126844406\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 19000: 6.62202358246\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 19500: 6.98678302765\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 20000: 7.61656808853\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 20500: 6.94073629379\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 21000: 6.88138151169\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 21500: 6.69609165192\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 22000: 6.72346687317\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 22500: 6.77591562271\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 23000: 7.06503534317\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 23500: 6.82861614227\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 24000: 7.82121658325\n",
      "Minibatch accuracy: 47.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 24500: 6.81899690628\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 25000: 6.26214313507\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 25500: 7.12920379639\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 26000: 6.57387447357\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 26500: 6.61694908142\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 27000: 5.81487417221\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 27500: 5.5076880455\n",
      "Minibatch accuracy: 66.25%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 28000: 5.55737495422\n",
      "Minibatch accuracy: 61.25%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 28500: 6.04742622375\n",
      "Minibatch accuracy: 58.75%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 29000: 6.62401819229\n",
      "Minibatch accuracy: 58.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 29500: 6.36558485031\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 30000: 7.27122688293\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 30500: 6.97510766983\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 31000: 6.33080911636\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 31500: 6.62284088135\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 32000: 8.01964473724\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 32500: 7.04086637497\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 33000: 7.09645652771\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 33500: 7.04881191254\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 34000: 7.44300031662\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 34500: 6.58228206635\n",
      "Minibatch accuracy: 58.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 35000: 7.23462963104\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 35500: 6.80219984055\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 36000: 6.80539464951\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 36500: 7.49810028076\n",
      "Minibatch accuracy: 47.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 37000: 6.9052491188\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 37500: 6.70181894302\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 38000: 7.41723251343\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 38500: 7.36638641357\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 39000: 6.45531368256\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 39500: 7.55118227005\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 40000: 7.25781440735\n",
      "Minibatch accuracy: 47.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 40500: 7.3589720726\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 41000: 6.73870849609\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 41500: 5.60503530502\n",
      "Minibatch accuracy: 61.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 42000: 5.98840379715\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 42500: 6.17141819\n",
      "Minibatch accuracy: 58.75%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 43000: 6.61748170853\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 43500: 6.66189575195\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 44000: 6.31499958038\n",
      "Minibatch accuracy: 58.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 44500: 6.99103975296\n",
      "Minibatch accuracy: 47.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 45000: 7.05956172943\n",
      "Minibatch accuracy: 47.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 45500: 7.03174972534\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 46000: 6.59898757935\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 46500: 6.77791500092\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 47000: 6.59788036346\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 47500: 6.98540878296\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 48000: 6.8461971283\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 48500: 7.25064897537\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 49000: 6.61942625046\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 49500: 7.55057907104\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 50000: 6.29326200485\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 50500: 6.90724658966\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 51000: 7.00488901138\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 51500: 7.15804290771\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 52000: 7.13630104065\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 52500: 6.3254365921\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 53000: 6.87616109848\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 53500: 6.7494802475\n",
      "Minibatch accuracy: 56.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 54000: 6.4831738472\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 54500: 6.66892242432\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 55000: 7.27860832214\n",
      "Minibatch accuracy: 51.25%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 55500: 6.72902202606\n",
      "Minibatch accuracy: 53.75%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 56000: 6.40837335587\n",
      "Minibatch accuracy: 61.25%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 56500: 5.47162866592\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 57000: 6.02572488785\n",
      "Minibatch accuracy: 63.75%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 57500: 6.68633508682\n",
      "Minibatch accuracy: 61.25%\n",
      "Validation accuracy: 59.0112596763%\n",
      "Minibatch loss at step 58000: 8.04625415802\n",
      "Minibatch accuracy: 47.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 58500: 6.92993974686\n",
      "Minibatch accuracy: 57.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 59000: 7.31868314743\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Minibatch loss at step 59500: 7.29842615128\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 57.4700914849%\n",
      "Test accuracy: 64.15365779%\n",
      "Model saved in file: model_trial_2.ckpt\n",
      "Successfully completed computation and iterations!\n",
      "To view Tensorboard's visualizations, please run 'tensorboard --logdir=log_trial_2' in your terminal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, h5py \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "print('Loading pickled data...')\n",
    "\n",
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    X_train = save['train_dataset']\n",
    "    y_train = save['train_labels']\n",
    "    X_val = save['valid_dataset']\n",
    "    y_val = save['valid_labels']\n",
    "    X_test = save['test_dataset']\n",
    "    y_test = save['test_labels']\n",
    "    del save  \n",
    "    print('Training data shape:', X_train.shape)\n",
    "    print('Training label shape:',y_train.shape)\n",
    "    print('Validation data shape:', X_val.shape)\n",
    "    print('Validation label shape:', y_val.shape)\n",
    "    print('Test data shape:', X_test.shape)\n",
    "    print('Test label shape:', y_test.shape)\n",
    "\n",
    "print('Data successfully loaded!')\n",
    "\n",
    "\n",
    "\n",
    "print('Defining accuracy function...')\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])\n",
    "print('Accuracy function defined!')\n",
    "\n",
    "# CNN Model\n",
    "print('Loading data and building computation graph...')\n",
    "\n",
    "'''Basic information'''\n",
    "# We processed image size to be 32\n",
    "image_size = 32\n",
    "# Number of channels: 1 because greyscale\n",
    "num_channels = 1\n",
    "# Mini-batch size\n",
    "batch_size = 16\n",
    "# Number of output labels\n",
    "num_labels = 11\n",
    "\n",
    "'''Filters'''\n",
    "# depth: number of filters (output channels) - should be increasing\n",
    "# num_channels: number of input channels set at 1 previously\n",
    "patch_size = 5\n",
    "depth_1 = 16\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 3\n",
    "\n",
    "# Number of hidden nodes in fully connected layer 1\n",
    "num_hidden = 64\n",
    "shape = [batch_size, image_size, image_size, num_channels]\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    '''Input Data'''\n",
    "    # X_train: (223965, 32, 32, 1)\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "\n",
    "    # y_train: (223965, 7)\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "\n",
    "    # X_val: (11788, 32, 32, 1)\n",
    "    tf_valid_dataset = tf.constant(X_val)\n",
    "\n",
    "    # X_test: (13067, 32, 32, 1)\n",
    "    tf_test_dataset = tf.constant(X_test)\n",
    "\n",
    "    '''Variables'''\n",
    "\n",
    "    # Create Variables Function\n",
    "    def init_weights_conv(shape, name):\n",
    "        return tf.get_variable(shape=shape, name=name, initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    def init_weights_fc(shape, name):\n",
    "        return tf.get_variable(shape=shape, name=name, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def init_biases(shape, name):\n",
    "        return tf.Variable(tf.constant(1.0, shape=shape), name=name)\n",
    "\n",
    "    # Create Function for Image Size: Pooling\n",
    "    # 3 Convolutions\n",
    "    # 2 Max Pooling\n",
    "    def output_size_pool(input_size, conv_filter_size, pool_filter_size, padding, conv_stride, pool_stride):\n",
    "        if padding == 'same':\n",
    "            padding = -1.00\n",
    "        elif padding == 'valid':\n",
    "            padding = 0.00\n",
    "        else:\n",
    "            return None\n",
    "        # After convolution 1\n",
    "        output_1 = (\n",
    "            ((input_size - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        # After pool 1\n",
    "        output_2 = (\n",
    "            ((output_1 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        # After convolution 2\n",
    "        output_3 = (\n",
    "            ((output_2 - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        # After pool 2\n",
    "        output_4 = (\n",
    "            ((output_3 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        # After convolution 2\n",
    "        output_5 = (\n",
    "            ((output_4 - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        # After pool 2\n",
    "        # output_6 = (\n",
    "        #     ((output_5 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        return int(output_5)\n",
    "\n",
    "    # Convolution 1\n",
    "    # Input channels: num_channels = 1\n",
    "    # Output channels: depth = depth_1\n",
    "    w_c1 = init_weights_conv([patch_size, patch_size, num_channels, depth_1], 'w_c1')\n",
    "    b_c1 = init_biases([depth_1], 'b_c1')\n",
    "\n",
    "    # Convolution 2\n",
    "    # Input channels: num_channels = depth_1\n",
    "    # Output channels: depth = depth_2\n",
    "    w_c2 = init_weights_conv([patch_size, patch_size, depth_1, depth_2], 'w_c2')\n",
    "    b_c2 = init_biases([depth_2], 'b_c2')\n",
    "\n",
    "    # Convolution 3\n",
    "    # Input channels: num_channels = depth_2\n",
    "    # Output channels: depth = depth_3\n",
    "    w_c3 = init_weights_conv([patch_size, patch_size, depth_2, depth_3], 'w_c3')\n",
    "    b_c3 = init_biases([depth_3], 'b_c3')\n",
    "\n",
    "    # Fully Connect Layer 1\n",
    "    final_image_size = output_size_pool(input_size=image_size,\n",
    "                                        conv_filter_size=5, pool_filter_size=2,\n",
    "                                        padding='valid', conv_stride=1,\n",
    "                                        pool_stride=2)\n",
    "    print('Final image size after convolutions {}'.format(final_image_size))\n",
    "    w_fc1 = init_weights_fc([final_image_size*final_image_size*depth_3, num_hidden], 'w_fc1')\n",
    "    b_fc1 = init_biases([num_hidden], 'b_fc1')\n",
    "\n",
    "    # Softmax 1\n",
    "    w_s1 = init_weights_fc([num_hidden, num_labels], 'w_s1')\n",
    "    b_s1 = init_biases([num_labels], 'b_s1')\n",
    "\n",
    "    # Softmax 2\n",
    "    w_s2 = init_weights_fc([num_hidden, num_labels], 'w_s2')\n",
    "    b_s2 = init_biases([num_labels], 'b_s2')\n",
    "\n",
    "    # Softmax 3\n",
    "    w_s3 = init_weights_fc([num_hidden, num_labels], 'w_s3')\n",
    "    b_s3 = init_biases([num_labels], 'b_s3')\n",
    "\n",
    "    # Softmax 4\n",
    "    w_s4 = init_weights_fc([num_hidden, num_labels], 'w_s4')\n",
    "    b_s4 = init_biases([num_labels], 'b_s4')\n",
    "\n",
    "    # Softmax 5\n",
    "    w_s5 = init_weights_fc([num_hidden, num_labels], 'w_s5')\n",
    "    b_s5 = init_biases([num_labels], 'b_s5')\n",
    "\n",
    "    def model(data, keep_prob, shape):\n",
    "        with tf.name_scope(\"conv_layer_1\"):\n",
    "            conv_1 = tf.nn.conv2d(data, w_c1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_1 = tf.nn.relu(conv_1 + b_c1)\n",
    "            pool_1 = tf.nn.max_pool(hidden_conv_1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        with tf.name_scope(\"conv_layer_2\"):\n",
    "            conv_2 = tf.nn.conv2d(pool_1, w_c2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_2 = tf.nn.relu(conv_2 + b_c2)\n",
    "            pool_2 = tf.nn.max_pool(hidden_conv_2, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        with tf.name_scope(\"conv_layer_3\"):\n",
    "            conv_3 = tf.nn.conv2d(pool_2, w_c3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_3 = tf.nn.relu(conv_3 + b_c3)\n",
    "        with tf.name_scope(\"fc_layer_1\"):\n",
    "            hidden_drop = tf.nn.dropout(hidden_conv_3, keep_prob)\n",
    "            shape = hidden_drop.get_shape().as_list()\n",
    "            reshape = tf.reshape(hidden_drop, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_fc = tf.nn.relu(tf.matmul(reshape, w_fc1) + b_fc1)\n",
    "        with tf.name_scope(\"softmax_1\"):\n",
    "            logits_1 = tf.matmul(hidden_fc, w_s1) + b_s1\n",
    "        with tf.name_scope(\"softmax_2\"):\n",
    "            logits_2 = tf.matmul(hidden_fc, w_s2) + b_s2\n",
    "        with tf.name_scope(\"softmax_3\"):\n",
    "            logits_3 = tf.matmul(hidden_fc, w_s3) + b_s3\n",
    "        with tf.name_scope(\"softmax_4\"):\n",
    "            logits_4 = tf.matmul(hidden_fc, w_s4) + b_s4\n",
    "        with tf.name_scope(\"softmax_5\"):\n",
    "            logits_5 = tf.matmul(hidden_fc, w_s5) + b_s5\n",
    "        return [logits_1, logits_2, logits_3, logits_4, logits_5]\n",
    "\n",
    "    '''Training Computation'''\n",
    "    [logits_1, logits_2, logits_3, logits_4, logits_5] = model(tf_train_dataset, 0.9, shape)\n",
    "\n",
    "    '''Loss Function'''\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_1, tf_train_labels[:, 1])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_2, tf_train_labels[:, 2])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_3, tf_train_labels[:, 3])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_4, tf_train_labels[:, 4])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_5, tf_train_labels[:, 5]))\n",
    "        # Add scalar summary for cost\n",
    "        tf.scalar_summary(\"loss\", loss)\n",
    "\n",
    "    '''Optimizer'''\n",
    "    # Decaying learning rate\n",
    "    # count the number of steps taken\n",
    "    global_step = tf.Variable(0)\n",
    "    start_learning_rate = 0.05\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 10000, 0.96)\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    '''Predictions'''\n",
    "    def softmax_combine(dataset, shape):\n",
    "        train_prediction = tf.pack([\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[0]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[1]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[2]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[3]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[4])])\n",
    "        return train_prediction\n",
    "\n",
    "    train_prediction = softmax_combine(tf_train_dataset, shape)\n",
    "    valid_prediction = softmax_combine(tf_valid_dataset, shape)\n",
    "    test_prediction = softmax_combine(tf_test_dataset, shape)\n",
    "\n",
    "    '''Save Model (will be initiated later)'''\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    '''Histogram for Weights'''\n",
    "    # Add histogram summaries for weights\n",
    "    tf.histogram_summary(\"w_c1_summ\", w_c1)\n",
    "    tf.histogram_summary(\"b_c1_summ\", b_c1)\n",
    "\n",
    "    tf.histogram_summary(\"w_c2_summ\", w_c2)\n",
    "    tf.histogram_summary(\"b_c2_summ\", b_c2)\n",
    "\n",
    "    tf.histogram_summary(\"w_c3_summ\", w_c3)\n",
    "    tf.histogram_summary(\"b_c3_summ\", b_c3)\n",
    "\n",
    "    tf.histogram_summary(\"w_fc1_summ\", w_fc1)\n",
    "    tf.histogram_summary(\"b_fc1_summ\", b_fc1)\n",
    "\n",
    "    tf.histogram_summary(\"w_s1_summ\", w_s1)\n",
    "    tf.histogram_summary(\"b_s1_summ\", b_s1)\n",
    "\n",
    "    tf.histogram_summary(\"w_s2_summ\", w_s2)\n",
    "    tf.histogram_summary(\"b_s2_summ\", b_s2)\n",
    "\n",
    "    tf.histogram_summary(\"w_s3_summ\", w_s3)\n",
    "    tf.histogram_summary(\"b_s3_summ\", b_s3)\n",
    "\n",
    "    tf.histogram_summary(\"w_s4_summ\", w_s4)\n",
    "    tf.histogram_summary(\"b_s4_summ\", b_s4)\n",
    "\n",
    "    tf.histogram_summary(\"w_s5_summ\", w_s5)\n",
    "    tf.histogram_summary(\"b_s5_summ\", b_s5)\n",
    "\n",
    "print('Data loaded and computation graph built!')\n",
    "\n",
    "num_steps = 60000\n",
    "\n",
    "print('Running computation and iteration...')\n",
    "print('If you are unable to save the summary, please change the path to where you want it to write.')\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.train.SummaryWriter(\"log_trial_2\", session.graph)  # for 0.8\n",
    "    merged = tf.merge_all_summaries()\n",
    "\n",
    "    '''If you want to restore model'''\n",
    "    # saver.restore(session, \"model_trial_1.ckpt\")\n",
    "    # print(\"Model restored!\")\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions, summary = session.run([optimizer, loss, train_prediction, merged], feed_dict=feed_dict)\n",
    "        writer.add_summary(summary)\n",
    "        if (step % 500 == 0):\n",
    "            print(('Minibatch loss at step {}: {}').format(step, l))\n",
    "            print(('Minibatch accuracy: {}%'.format(accuracy(predictions, batch_labels[:,1:6]))))\n",
    "            print(('Validation accuracy: {}%'.format(accuracy(valid_prediction.eval(), y_val[:,1:6]))))\n",
    "    print(('Test accuracy: {}%'.format(accuracy(test_prediction.eval(), y_test[:,1:6]))))\n",
    "\n",
    "    save_path = saver.save(session, \"model_trial_2.ckpt\")\n",
    "    print('Model saved in file: {}'.format(save_path))\n",
    "\n",
    "\n",
    "print('Successfully completed computation and iterations!')\n",
    "\n",
    "print('To view Tensorboard\\'s visualizations, please run \\'tensorboard --logdir=log_trial_2\\' in your terminal')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p]",
   "language": "python",
   "name": "conda-env-p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
