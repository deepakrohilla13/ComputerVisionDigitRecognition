{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, h5py \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "pickle_file = 'SVHN.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training data shape:', (230070, 32, 32, 1))\n",
      "('Training label shape:', (230070, 6))\n",
      "('Validation data shape:', (5684, 32, 32, 1))\n",
      "('Validation label shape:', (5684, 6))\n",
      "('Test data shape:', (13068, 32, 32, 1))\n",
      "('Test label shape:', (13068, 6))\n"
     ]
    }
   ],
   "source": [
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    X_train = save['train_dataset']\n",
    "    y_train = save['train_labels']\n",
    "    X_val = save['valid_dataset']\n",
    "    y_val = save['valid_labels']\n",
    "    X_test = save['test_dataset']\n",
    "    y_test = save['test_labels']\n",
    "    del save  \n",
    "    print('Training data shape:', X_train.shape)\n",
    "    print('Training label shape:',y_train.shape)\n",
    "    print('Validation data shape:', X_val.shape)\n",
    "    print('Validation label shape:', y_val.shape)\n",
    "    print('Test data shape:', X_test.shape)\n",
    "    print('Test label shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels)/ predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_channels = 1\n",
    "batch_size = 16\n",
    "num_labels = 11\n",
    "patch_size = 5\n",
    "depth_1 = 16\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 3\n",
    "num_hidden = 64\n",
    "shape = [batch_size, image_size, image_size, num_channels]\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "    tf_valid_dataset = tf.constant(X_val)\n",
    "    tf_test_dataset = tf.constant(X_test)\n",
    "    def init_weights(shape, name):\n",
    "        return tf.Variable(tf.random_normal(shape=shape, stddev=0.01),name=name)\n",
    "\n",
    "    def init_biases(shape, name):\n",
    "        return tf.Variable(tf.constant(1.0, shape=shape),name=name)\n",
    "\n",
    "    def output_size_pool(input_size, conv_filter_size, pool_filter_size, padding, conv_stride, pool_stride):\n",
    "        if padding == 'same':\n",
    "            padding = -1.00\n",
    "        elif padding == 'valid':\n",
    "            padding = 0.00\n",
    "        else:\n",
    "            return None\n",
    "        output_1 = (((input_size - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        output_2 = (((output_1 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        output_3 = (((output_2 - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        output_4 = (((output_3 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        output_5 = (((output_4 - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        return int(output_5)\n",
    "\n",
    "    w_c1 = init_weights([patch_size, patch_size, num_channels, depth_1], 'w_c1')\n",
    "    b_c1 = init_biases([depth_1], 'b_c1')\n",
    "    w_c2 = init_weights([patch_size, patch_size, depth_1, depth_2], 'w_c2')\n",
    "    b_c2 = init_biases([depth_2], 'b_c2')\n",
    "    w_c3 = init_weights([patch_size, patch_size, depth_2, depth_3], 'w_c3')\n",
    "    b_c3 = init_biases([depth_3], 'b_c3')\n",
    "    final_image_size = output_size_pool(input_size=image_size,conv_filter_size=5, pool_filter_size=2,padding='valid', conv_stride=1,pool_stride=2)\n",
    "    w_fc1 = init_weights([final_image_size*final_image_size*depth_3, num_hidden], 'w_fc1')\n",
    "    b_fc1 = init_biases([num_hidden], 'b_fc1')\n",
    "    w_s1 = init_weights([num_hidden, num_labels], 'w_s1')\n",
    "    b_s1 = init_biases([num_labels], 'b_s1')\n",
    "    w_s2 = init_weights([num_hidden, num_labels], 'w_s2')\n",
    "    b_s2 = init_biases([num_labels], 'b_s2')\n",
    "    w_s3 = init_weights([num_hidden, num_labels], 'w_s3')\n",
    "    b_s3 = init_biases([num_labels], 'b_s3')\n",
    "    w_s4 = init_weights([num_hidden, num_labels], 'w_s4')\n",
    "    b_s4 = init_biases([num_labels], 'b_s4')\n",
    "    w_s5 = init_weights([num_hidden, num_labels], 'w_s5')\n",
    "    b_s5 = init_biases([num_labels], 'b_s5')\n",
    "\n",
    "    def model(data, keep_prob, shape):\n",
    "        with tf.name_scope(\"conv_layer_1\"):\n",
    "            conv_1 = tf.nn.conv2d(data, w_c1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_1 = tf.nn.relu(conv_1 + b_c1)\n",
    "            pool_1 = tf.nn.max_pool(hidden_conv_1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        with tf.name_scope(\"conv_layer_2\"):\n",
    "            conv_2 = tf.nn.conv2d(pool_1, w_c2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_2 = tf.nn.relu(conv_2 + b_c2)\n",
    "            pool_2 = tf.nn.max_pool(hidden_conv_2, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        with tf.name_scope(\"conv_layer_3\"):\n",
    "            conv_3 = tf.nn.conv2d(pool_2, w_c3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_3 = tf.nn.relu(conv_3 + b_c3)\n",
    "        with tf.name_scope(\"fc_layer_1\"):\n",
    "            hidden_drop = tf.nn.dropout(hidden_conv_3, keep_prob)\n",
    "            shape = hidden_drop.get_shape().as_list()\n",
    "            reshape = tf.reshape(hidden_drop, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_fc = tf.nn.relu(tf.matmul(reshape, w_fc1) + b_fc1)\n",
    "        with tf.name_scope(\"softmax_1\"):\n",
    "            logits_1 = tf.matmul(hidden_fc, w_s1) + b_s1\n",
    "        with tf.name_scope(\"softmax_2\"):\n",
    "            logits_2 = tf.matmul(hidden_fc, w_s2) + b_s2\n",
    "        with tf.name_scope(\"softmax_3\"):\n",
    "            logits_3 = tf.matmul(hidden_fc, w_s3) + b_s3\n",
    "        with tf.name_scope(\"softmax_4\"):\n",
    "            logits_4 = tf.matmul(hidden_fc, w_s4) + b_s4\n",
    "        with tf.name_scope(\"softmax_5\"):\n",
    "            logits_5 = tf.matmul(hidden_fc, w_s5) + b_s5\n",
    "        return [logits_1, logits_2, logits_3, logits_4, logits_5]\n",
    "\n",
    "    [logits_1, logits_2, logits_3, logits_4, logits_5] = model(tf_train_dataset, 0.5, shape)\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        reduce_mean_logits1_digit1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_1, tf_train_labels[:, 1]))\n",
    "        reduce_mean_logits2_digit2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_2, tf_train_labels[:, 2]))\n",
    "        reduce_mean_logits3_digit3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_3, tf_train_labels[:, 3]))\n",
    "        reduce_mean_logits4_digit4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_4, tf_train_labels[:, 4]))\n",
    "        reduce_mean_logits5_digit5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_5, tf_train_labels[:, 5]))\n",
    "        \n",
    "        loss = reduce_mean_logits1_digit1 + reduce_mean_logits2_digit2 + reduce_mean_logits3_digit3 + reduce_mean_logits4_digit4 + reduce_mean_logits5_digit5\n",
    "\n",
    "    global_step = tf.Variable(0)\n",
    "    start_learning_rate = 0.05\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    '''Predictions'''\n",
    "    def softmax_combine(dataset, shape):\n",
    "        logit_list = model(dataset, 1.0, shape)\n",
    "        model_logit_0 = logit_list[0]\n",
    "        model_logit_1 = logit_list[1]\n",
    "        model_logit_2 = logit_list[2]\n",
    "        model_logit_3 = logit_list[3]\n",
    "        model_logit_4 = logit_list[4]\n",
    "        train_prediction = tf.pack([\n",
    "                                    tf.nn.softmax(model_logit_0),\n",
    "                                    tf.nn.softmax(model_logit_1),\n",
    "                                    tf.nn.softmax(model_logit_2),\n",
    "                                    tf.nn.softmax(model_logit_3),\n",
    "                                    tf.nn.softmax(model_logit_4)])\n",
    "        return train_prediction\n",
    "\n",
    "    train_prediction = softmax_combine(tf_train_dataset, shape)\n",
    "    valid_prediction = softmax_combine(tf_valid_dataset, shape)\n",
    "    test_prediction = softmax_combine(tf_test_dataset, shape)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-5ed94d0c038e>:2 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:From <ipython-input-7-5ed94d0c038e>:3 in <module>.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From <ipython-input-7-5ed94d0c038e>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <type 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ed94d0c038e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf_train_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch loss at step {}: {}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/p/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/p/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/p/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \"\"\"\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/p/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/p/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/p/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[0;32m--> 227\u001b[0;31m                       (fetch, type(fetch)))\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <type 'NoneType'>"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.train.SummaryWriter(\"log_trial_3\", session.graph)  # for 0.8\n",
    "    merged = tf.merge_all_summaries()\n",
    "    # saver.restore(session, \"model_trial_1.ckpt\")\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data,tf_train_labels: batch_labels}\n",
    "        _, l, predictions, summary = session.run([optimizer, loss, train_prediction, merged],feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(('Minibatch loss at step {}: {}').format(step, l))\n",
    "            print(('Minibatch accuracy: {}%'.format(accuracy(predictions, batch_labels[:,1:6]))))\n",
    "            print(('Validation accuracy: {}%'.format(accuracy(valid_prediction.eval(),y_val[:,1:6]))))\n",
    "    print(('Test accuracy: {}%'.format(accuracy(test_prediction.eval(), y_test[:,1:6]))))\n",
    "    save_path = saver.save(session, \"model_trial_1.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p]",
   "language": "python",
   "name": "conda-env-p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
